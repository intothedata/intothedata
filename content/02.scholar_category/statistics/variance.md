---
title: "분산 - Variance"
categories: [statistics,통계]
tags: [분산,variance]
---

## 개요

분산은 데이터가 평균으로 부터 얼마나 많이 떨어져 있는지를 나타내는 통계값이다.  분산은 표준편차의 제곱값과 동일하고 표준편차는 분산의 제곱근(sqrt)이다.  표준편차와 분산은 사실 동일하다고 볼 수 있는데 표준편차보다는 표준편차가 제곱된 형태의 분산이 더 많이 쓰인다. 

### 표준편차보다 분산을 더 많이 사용하는 이유

1. 제곱이 된 상태를 두면 후에 미분을 적용하거나 여러 계산에서 분산값을 계산할 때 편해지는 이점이 있다. 분산의 값을 재계산하는 수식들은 통계학이나 공학의 논문이나 서적에서 볼 수 있는데 그런 수식들 중에서 분산이 제곱근이 되어 있지 않아 편리하게 계산이 되는 것을 볼 수 있다.

2. 분산은 제곱이 된 상태이기 때문에 개별 숫자의 평균으로부터의 편차가 클수록 더 커지는 효과를 주게된다. 즉, 평균에서 거리가 더 먼 숫자일 수록 편차에 더 가중치를 주어 계산된 상태가 되는 이는 데이터분석에서 데이터의 분포나 패턴을 볼 때 더 뚜렷하게 볼 수 있게 해주기도 한다.

### 분산의 공식

샘플 분산(모분산이 아닌 샘플링한 데이터의 분산)의 공식은 다음과 같다.  샘플 분산의 공식과 모집단의 분산인 모분산을 구하는 공식은 조금 다르다.

$${{\sum {{{(x - \bar x)}^2}}} \over {n - 1}}$$

샘플 분산을 구할 때 분모의 $n -1$ 부분이 다른데 $n - 1$은 전체 샘플수에서 1개를 뺀 것이고 이것은 자유도때문이다. 자유도의 문제를 다 이해하지 않더라도 1을 빼주지 않으면 적은 수의 샘플에서 구한 분산과 표준편차값이 실제 모집단의 분산과 표준편차를 반영하지 못하고 오차가 발생한다는 것으로 알려져 있다.

$n - 1$로 인해 샘플수가 적거나 많거나와 상관없이 샘플의 분산이 최대한 모집단의 분산에 오차없이 가깝게 해준다.

> 자세한 것은 자유도에 대한 내용을 참조하기 바란다.

## 효율적으로 분산 구하기

분산을 구하는 수식을 보면 계산을 할때 평균값을 계산에서 사용하므로 먼저 수열로부터 평균을 구한 후에 수열을 다시 한 번 더 계산이 가능하다.

분산을 구하는 수식

<div>
    $$
    {s^2} = \frac{{\sum\limits_{i = 1}^N {(x_i^2 - 2\bar x{x_i} + {{\bar x}^2})} }}{{N - 1}} = \frac{{\sum\limits_{i = 1}^N {x_i^2 - 2N{{\bar x}^2} + N{{\bar x}^2}} }}{{N - 1}} = \frac{{\sum {x_i^2 - N{{\bar x}^2}} }}{{N - 1}}
    $$
</div>

위의 식은 일반식으로 보통이라면 큰 문제가 없다. 하지만 수백억개의 수열로 딘 데이터의 분산을 구해야 하면 수치가 너무 커서 컴퓨터의 변수가 사용할 수 있는 한계 메모리를 초과해서 에러를 발생시킬 수도 있고 값을 두 번 반복해서 읽기 때문에 낭비가 된다.

Welford's method 라는 방식을 사용하면 수열을 한번만 반복해서 읽는 것으로 이 문제를 해결할 수 있다. 수식을 풀어보면 사실 분산을 구하기 위해서 수열을 두 번 스캔할 필요가 없이 한 번의 스캔으로 계산이 가능하다. 만약 Hadoop을 이용해 Map/Reduce를 작성해서 천만개의 수치값에 대한 분산을 구한다고 하면 (사실 실제로 그럴일이 별로 없다) 변형된 수식을 이용한 한 번의 Map/Reduce로도 분산을 구하는 것이 가능해진다.

<div>
    $$ (N - 1)s_N^2 - (N - 2)s_{N - 1}^2 \\
    = \sum\limits_{i = 1}^N {{{({x_i} - {{\bar x}_N})}^2}}  - \sum\limits_{i = 1}^{N - 1} {{{({x_i} - {{\bar x}_{N - 1}})}^2}} \\

    = {({x_N} - {{\bar x}_N})^2} + \sum\limits_{i = 1}^{N - 1} {\left( {{{({x_i} - {{\bar x}_N})}^2} - {{({x_i} - {{\bar x}_{N - 1}})}^2}} \right)} \\

    = {({x_N} - {{\bar x}_N})^2} + \sum\limits_{i = 1}^{N - 1} {({x_i} - {{\bar x}_N} + {x_i} - {{\bar x}_{N - 1}})} ({{\bar x}_{N - 1}} - {{\bar x}_N}) \\

    = {({x_N} - {{\bar x}_N})^2} + ({{\bar x}_N} - {x_N})({{\bar x}_{N - 1}}?{{\bar x}_N}) \\
    = ({x_N} - {{\bar x}_N})({x_N} - {{\bar x}_N} - {{\bar x}_{N - 1}} + {{\bar x}_N}) \\
    = ({x_N} - {{\bar x}_N})({x_N} - {{\bar x}_{N - 1}}) $$
</div>

위의 전개는 매우 복잡해 보이지만 결국 필요한 것은 마지막 수식이다.

<div>$$
({x_N} - {{\bar x}_N}) ({x_N} - {{\bar x}_{N - 1}}) 
$$</div>

위의 수식으로 분산을 계산하는 것을 구현한다면 분산을 한번의 반복(iteration)으로 구할 수 있다.
